{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmojiNetTutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C2bxyl4KAsD"
      },
      "source": [
        "### **[INFO] Make sure to run on the GPU runtime type!**\n",
        "\n",
        "<img src=\"https://www.kdnuggets.com/wp-content/uploads/colab-settings-1.png\" alt=\"Drawing\" style=\"width: 300px; border: 1px solid red\" width=300/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC6y1mU2murQ"
      },
      "source": [
        "# Prepare the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2I3W8WcnGRR",
                "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ialhashim/EmojiNetTutorial/blob/master/EmojiNetTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C2bxyl4KAsD",
        "colab_type": "text"
      },
      "source": [
        "dataset_size = 1000\n",
        "num_datasets = 1\n",
        "\n",
        "print('Will generate', dataset_size * num_datasets, 'samples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install imagemagick libmagickwand-dev libmagickcore-6.q16-3-extra\n",
        "!pip install Wand"
      ],
      "metadata": {
        "id": "GtuDtoYxGj6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu1Gn0hv84t7"
      },
      "source": [
        "%pylab inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD-4MJQfnTen"
      },
      "source": [
        "#\n",
        "# 2) Prepare input and output data folders\n",
        "#\n",
        "import os\n",
        "os.makedirs('emoji', exist_ok=True)\n",
        "os.makedirs('png', exist_ok=True)\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "#\n",
        "# 3) Download Emoji SVG files, create high-res PNGs\n",
        "#\n",
        "\n",
        "def emoji_svg_file(emoji_filename):\n",
        "  return './emoji/'+emoji_filename+'.svg'\n",
        "\n",
        "def emoji_png_file(emoji_filename):\n",
        "  return './png/'+emoji_filename+'.png'\n",
        "\n",
        "def download_emoji_svg(emoji_filename):\n",
        "  from urllib import request\n",
        "  import os.path\n",
        "  emoji_path = 'emoji/'+emoji_filename+'.svg'\n",
        "  f = open(emoji_path, 'wb')\n",
        "  f.write(request.urlopen('https://raw.githubusercontent.com/googlefonts/noto-emoji/main/svg/'+emoji_filename+'.svg').read())\n",
        "  f.close()\n",
        "  print('Downloaded: ' + emoji_filename)\n",
        "  \n",
        "def download_all_emoji(emoji_set):\n",
        "  for key in emoji_set.keys():\n",
        "    download_emoji_svg(emoji_set[key])\n",
        "    \n",
        "def convert_svg_to_png(emoji_set):\n",
        "  import os\n",
        "  for key in emoji_set.keys():\n",
        "    filename = emoji_set[key]\n",
        "    cmd = ''\n",
        "    if os.name == 'nt':\n",
        "      cmd = 'magick convert -density 384 -background none emoji/'+filename+'.svg png/'+filename+'.png'\n",
        "      os.system(cmd)\n",
        "    else:\n",
        "      cmd = 'convert -density 384 -background none emoji/'+filename+'.svg png/'+filename+'.png'\n",
        "      os.system(cmd)\n",
        "    print('Converted ' + filename, cmd)\n",
        "    \n",
        "location = {\n",
        "\t\t'camping': 'emoji_u1f3d5',\n",
        "\t\t'beach': 'emoji_u1f3d6',\n",
        "\t\t'desert': 'emoji_u1f3dc',\n",
        "\t\t'park': 'emoji_u1f3de',\n",
        "\t\t'factory': 'emoji_u1f3ed',\n",
        "\t\t'hills': 'emoji_u1f304',\n",
        "\t\t'city': 'emoji_u1f307',\n",
        "\t\t'golf': 'emoji_u26f3',\n",
        "\t\t'night': 'emoji_u1f306'}\n",
        "\n",
        "actor = {\n",
        "\t\t'man_walk': 'emoji_u1f6b6_1f3fe_200d_2642',\n",
        "\t\t'man_run': 'emoji_u1f3c3_1f3fb_200d_2642',\n",
        "\t\t'man_sport': 'emoji_u1f3cb_1f3fb_200d_2642',\n",
        "\t\t'man_police': 'emoji_u1f46e_1f3fd_200d_2642',\n",
        "\t\t'man_hello': 'emoji_u1f64b_1f3fe_200d_2642',\n",
        "\t\t'man_engineer': 'emoji_u1f468_1f3fc_200d_1f527',\n",
        "\t\t'man_turban': 'emoji_u1f473_1f3fe_200d_2642',\n",
        "\t\t'man_suit': 'emoji_u1f935_1f3fe',\n",
        "\t\t'man_clown': 'emoji_u1f939_1f3fb_200d_2642',\n",
        "\t\t'man_construction': 'emoji_u1f477_200d_2642',\n",
        "\t\t'man_doctor': 'emoji_u1f468_1f3ff_200d_2695',\n",
        "\t\t'women_bicycle': 'emoji_u1f6b4_1f3fb_200d_2640',\n",
        "\t\t'women_hijab': 'emoji_u1f9d5_1f3fc',\n",
        "\t\t'women_doctor': 'emoji_u1f469_1f3fc_200d_2695',\n",
        "\t\t'animal_camel': 'emoji_u1f42a',\n",
        "\t\t'animal_cat': 'emoji_u1f408',\n",
        "\t\t'animal_bird': 'emoji_u1f426',\n",
        "\t\t'animal_tree': 'emoji_u1f333',\n",
        "\t\t'animal_car': 'emoji_u1f697',\n",
        "\t\t'animal_bus': 'emoji_u1f68c'}\n",
        "\n",
        "danger = {\n",
        "\t\t'danger1': 'emoji_u1f4a3',\n",
        "\t\t'danger2': 'emoji_u1f52b',\n",
        "\t\t'danger3': 'emoji_u1f52a',\n",
        "\t\t'danger4': 'emoji_u2622'}\n",
        "\n",
        "# Download all Emoji files\n",
        "download_all_emoji(location)\n",
        "download_all_emoji(actor)\n",
        "download_all_emoji(danger)\n",
        "\n",
        "# Create high res pngs\n",
        "convert_svg_to_png(location)\n",
        "convert_svg_to_png(actor)\n",
        "convert_svg_to_png(danger)\n",
        "\n",
        "# test resulting PNG\n",
        "#from google.colab import files\n",
        "#files.download('png/emoji_u1f697.png')\n",
        "\n",
        "from random import random\n",
        "from math import cos, sin, floor, sqrt, pi, ceil\n",
        "\n",
        "def euclidean_distance(a, b):\n",
        "    dx = a[0] - b[0]\n",
        "    dy = a[1] - b[1]\n",
        "    return sqrt(dx * dx + dy * dy)\n",
        "  \n",
        "# https://github.com/emulbreh/bridson\n",
        "def poisson_disc_samples(width, height, r, k=5, distance=euclidean_distance, random=random):\n",
        "    tau = 2 * pi\n",
        "    cellsize = r / sqrt(2)\n",
        "\n",
        "    grid_width = int(ceil(width / cellsize))\n",
        "    grid_height = int(ceil(height / cellsize))\n",
        "    grid = [None] * (grid_width * grid_height)\n",
        "\n",
        "    def grid_coords(p):\n",
        "        return int(floor(p[0] / cellsize)), int(floor(p[1] / cellsize))\n",
        "\n",
        "    def fits(p, gx, gy):\n",
        "        yrange = list(range(max(gy - 2, 0), min(gy + 3, grid_height)))\n",
        "        for x in range(max(gx - 2, 0), min(gx + 3, grid_width)):\n",
        "            for y in yrange:\n",
        "                g = grid[x + y * grid_width]\n",
        "                if g is None:\n",
        "                    continue\n",
        "                if distance(p, g) <= r:\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "    p = width * random(), height * random()\n",
        "    queue = [p]\n",
        "    grid_x, grid_y = grid_coords(p)\n",
        "    grid[grid_x + grid_y * grid_width] = p\n",
        "\n",
        "    while queue:\n",
        "        qi = int(random() * len(queue))\n",
        "        qx, qy = queue[qi]\n",
        "        queue[qi] = queue[-1]\n",
        "        queue.pop()\n",
        "        for _ in range(k):\n",
        "            alpha = tau * random()\n",
        "            d = r * sqrt(3 * random() + 1)\n",
        "            px = qx + d * cos(alpha)\n",
        "            py = qy + d * sin(alpha)\n",
        "            if not (0 <= px < width and 0 <= py < height):\n",
        "                continue\n",
        "            p = (px, py)\n",
        "            grid_x, grid_y = grid_coords(p)\n",
        "            if not fits(p, grid_x, grid_y):\n",
        "                continue\n",
        "            queue.append(p)\n",
        "            grid[grid_x + grid_y * grid_width] = p\n",
        "    return [p for p in grid if p is not None]\n",
        "\n",
        "# Draw a gradient\n",
        "def interpolate_color(minval, maxval, val, color_palette):\n",
        "  max_index = len(color_palette)-1\n",
        "  v = float(val-minval) / float(maxval-minval) * max_index\n",
        "  i1, i2 = int(v), min(int(v)+1, max_index)\n",
        "  (r1, g1, b1), (r2, g2, b2) = color_palette[i1], color_palette[i2]\n",
        "  f = v - i1\n",
        "  return int(r1 + f*(r2-r1)), int(g1 + f*(g2-g1)), int(b1 + f*(b2-b1))\n",
        "\n",
        "def draw_vt_gradient(draw, rect, color_func, color_palette):\n",
        "  (max_x, max_y) = rect\n",
        "  minval, maxval = 1, len(color_palette)\n",
        "  delta = maxval - minval\n",
        "  for y in range(0, max_y+1):\n",
        "    f = y / float(max_y)\n",
        "    val = minval + f * delta\n",
        "    color = color_func(minval, maxval, val, color_palette)\n",
        "    draw.line([(0, y), (max_x, y)], fill=color)\n",
        "\n",
        "# Create a sky graident image\n",
        "def create_sky_gradient_image(size):\n",
        "  from PIL import Image, ImageDraw\n",
        "  BLUE, WHITE, WHITE = ((28, 146, 210), (255, 255, 255), (255, 255, 255))\n",
        "  image = Image.new(\"RGB\", size)\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  draw_vt_gradient(draw, size, interpolate_color, [BLUE, WHITE, WHITE])\n",
        "  return image\n",
        "\n",
        "# Generate random locations filled with random actors\n",
        "def generate_random_location(locations, actors, dangers, count, s = 126):\n",
        "  import time\n",
        "  t = time.time()\n",
        "\n",
        "  from PIL import Image, ImageDraw, ImageFont\n",
        "  import random    \n",
        "\n",
        "  # Cache backgrounds\n",
        "  cached_locations = []\n",
        "  for key in locations.keys():\n",
        "    bg = Image.open(emoji_png_file(locations[key]))\n",
        "    cached_locations.append(bg)\n",
        "    cached_locations.append(bg.transpose(Image.FLIP_LEFT_RIGHT))\n",
        "\n",
        "  # Generate a background\n",
        "  backgrounds = []\n",
        "  for i in range(count):\n",
        "    margin = int(s*0.1)\n",
        "    location_img = cached_locations[random.randint(0, len(cached_locations)-1)]\n",
        "    location_img = location_img.resize((s+margin,s+margin), Image.BILINEAR)\n",
        "    img = create_sky_gradient_image((s,s))\n",
        "    img.paste(location_img,(int(-margin/2),int(-margin/2)),location_img)\n",
        "    \n",
        "    backgrounds.append(img)\n",
        "    \n",
        "  # Cache actors\n",
        "  cached_actors = []\n",
        "  actor_size = int(s * 0.3)\n",
        "  for key in actors.keys():\n",
        "    fg = Image.open(emoji_png_file(actors[key])).resize((actor_size,actor_size), Image.BILINEAR)\n",
        "    cached_actors.append(fg)\n",
        "    #cached_actors.append(fg.transpose(Image.FLIP_LEFT_RIGHT))\n",
        "  \n",
        "  # Generate some nice random sampling coordinates\n",
        "  samples = []\n",
        "  for i in range(10):\n",
        "    samples.append(poisson_disc_samples(s,s,int(s * 0.2)))\n",
        "  \n",
        "  # Add actors to the backgrounds\n",
        "  imgs = []\n",
        "  for bg in backgrounds:\n",
        "    positions = samples[random.randint(0, len(samples)-1)]\n",
        "    for p in positions:\n",
        "      if p[1] < s * 0.4: continue\n",
        "      actor_img = cached_actors[random.randint(0, len(cached_actors)-1)]\n",
        "      bg.paste(actor_img, (int(p[0]-actor_size/2),int(p[1]-actor_size/2)), actor_img)\n",
        "    imgs.append(bg)\n",
        "  \n",
        "  # Load and cache dangerous items\n",
        "  cached_danger = []\n",
        "  danger_size = int(actor_size)\n",
        "  for key in dangers:\n",
        "    d = Image.open(emoji_png_file(dangers[key])).resize((danger_size,danger_size), Image.BILINEAR)\n",
        "    cached_danger.append(d)\n",
        "  \n",
        "  # Add a dangerous item\n",
        "  masks = []\n",
        "  for img in imgs:\n",
        "    # Get a random location in the bottom half of the screen\n",
        "    positions = samples[random.randint(0, len(samples)-1)]\n",
        "    good_positions = []\n",
        "    for p in positions:\n",
        "      if p[1] > s * 0.4:\n",
        "        good_positions.append(p)\n",
        "    if len(good_positions) == 0: good_positions = positions\n",
        "    p = good_positions[random.randint(0, len(good_positions)-1)]\n",
        "    item = cached_danger[random.randint(0, len(cached_danger)-1)]\n",
        "    \n",
        "    img.paste(item, (int(p[0]-danger_size/2),int(p[1]-danger_size/2)), item)\n",
        "    \n",
        "    # Simulate CCTV footage by drawing some text\n",
        "    d = ImageDraw.Draw(img)\n",
        "    d.text((10,10), time.strftime(\"%Y-%m-%d %H:%M\"), fill=(255,255,255,128))\n",
        "    d.text((10,20), time.strftime(\"Camera 07\"), fill=(255,255,255,128))\n",
        "    \n",
        "  \n",
        "    mask = Image.new(\"L\", (s,s))\n",
        "    mask.paste((255), (int(p[0]-danger_size/2),int(p[1]-danger_size/2)), item)\n",
        "    masks.append(mask)\n",
        "    \n",
        "  elapsed = time.time() - t\n",
        "  print(\"(\"+str(len(imgs))+\") Scenes created in (\" + str(round(elapsed,3)) + \" s).\")\n",
        "\n",
        "  return imgs, masks\n",
        "\n",
        "# Test generation function\n",
        "imgs, masks = generate_random_location(location, actor, danger, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rglbj7Hp5hVo"
      },
      "source": [
        "figure(figsize=(8,8)); imshow(imgs[0])\n",
        "figure(figsize=(8,8)); imshow(masks[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZoyTQCx5FwC"
      },
      "source": [
        "# Save to disk\n",
        "from os.path import join\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "for di in range(num_datasets):\n",
        "  h5_dataset_filename = join('data', 'dataset'+f'{di:03}'+'.hdf5')\n",
        "  print(\"Creating [\"+h5_dataset_filename+\"] ...\")\n",
        "\n",
        "  with h5py.File(h5_dataset_filename, \"w\") as dataset:\n",
        "    # Generate large dataset\n",
        "    count = dataset_size\n",
        "    imgs, masks = generate_random_location(location, actor, danger, count)\n",
        "\n",
        "    for i in range(0, len(imgs)):\n",
        "      img, mask = imgs[i], masks[i]\n",
        "      filename = f'{i:05}'\n",
        "      dataset.create_dataset(filename + \".rgb\", data=np.array(img), compression=\"gzip\", compression_opts=9)\n",
        "      dataset.create_dataset(filename + \".mask\", data=np.array(mask), compression=\"gzip\", compression_opts=9)\n",
        "      \n",
        "  print(\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f38aMuzivq9Q"
      },
      "source": [
        "# Train the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSwrbs-5vrWn"
      },
      "source": [
        "#========================================================================================\n",
        "# Hyperparameters\n",
        "#========================================================================================\n",
        "mynetname           = 'emojinet'\n",
        "resolution          = 128\n",
        "epochs              = 5\n",
        "batch_size          = 64\n",
        "lrate               = 0.0001\n",
        "validation_split    = 0.2\n",
        "\n",
        "eN                  = 32\n",
        "dN                  = 64\n",
        "\n",
        "# Specify GPUs\n",
        "#import os\n",
        "#os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
        "\n",
        "#========================================================================================\n",
        "# Input\n",
        "#========================================================================================\n",
        "import os\n",
        "import cv2, h5py\n",
        "import numpy as np\n",
        "from urllib import request\n",
        "def load(resolution,count=10):\n",
        "    x,y = [],[]\n",
        "\n",
        "    for di in range(count):  \n",
        "        dataset_filename = './data/dataset'+f'{di:03}'+'.hdf5'\n",
        "\n",
        "        # Open dataset and collect all data\n",
        "        print(\"Loading: \" + dataset_filename)\n",
        "        with h5py.File(dataset_filename, \"r\") as dataset:\n",
        "            keys = []\n",
        "            for key in dataset.keys(): keys.append(key.split(\".\")[0])\n",
        "            keys = list(set(keys))\n",
        "\n",
        "            for key in keys:\n",
        "                rgb, mask = dataset.get(key+'.rgb')[()], dataset.get(key+'.mask')[()]\n",
        "\n",
        "                rgb = cv2.resize(rgb, dsize=(resolution, resolution))\n",
        "                mask = cv2.resize(mask, dsize=(resolution, resolution))\n",
        "\n",
        "                x.append(rgb)\n",
        "                y.append(mask.reshape((mask.shape[0],mask.shape[1],1)))\n",
        "\n",
        "    x = np.stack(x)\n",
        "    y = np.stack(y)\n",
        "\n",
        "    return x,y\n",
        "\n",
        "import numpy\n",
        "x,y = load(resolution=resolution, count=1)\n",
        "\n",
        "# Modify inputs\n",
        "x = x / 255\n",
        "y = y / 255\n",
        "\n",
        "# Augment\n",
        "\n",
        "print('x: ' + str(len(x)) + ' images of shape ' + str(x[0].shape))\n",
        "print('y: ' + str(len(y)) + ' images of shape ' + str(y[0].shape))\n",
        "\n",
        "#========================================================================================\n",
        "# Outputs\n",
        "#========================================================================================\n",
        "import time, pathlib\n",
        "\n",
        "# Define output folder name\n",
        "runID = str(int(time.time())) + '-n' + str(len(x)) + '-r' + str(resolution) + \\\n",
        "\t\t'-eN' + str(eN) + '-dN' + str(dN) + '-e' + str(epochs) + \\\n",
        "\t\t'-bs' + str(batch_size) + '-lr' + str(lrate) + '-' + mynetname\n",
        "\n",
        "runPath = './Graphs/' + runID\n",
        "\n",
        "print('Output: ' + runPath)\n",
        "pathlib.Path(runPath).mkdir(parents=True, exist_ok=True) \n",
        "pathlib.Path(runPath+'/checkpoints').mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(runPath+'/samples').mkdir(parents=True, exist_ok=True) \n",
        "\n",
        "#========================================================================================\n",
        "# Utils\n",
        "#========================================================================================\n",
        "import numpy, cv2\n",
        "\n",
        "# Predict for a single image\n",
        "def predict(model, image):\n",
        "    prediction = model.predict(image.reshape(1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    return numpy.tile(prediction,3)\n",
        "\n",
        "# Save image to PNG\n",
        "def save_png(image, filename):\n",
        "    if image.shape[2] == 3:\n",
        "        image = cv2.cvtColor(image.astype(numpy.uint8), cv2.COLOR_RGB2BGR)\n",
        "    cv2.imwrite(filename, numpy.clip(image,0,255))\n",
        "\n",
        "# Loss function for UNet\n",
        "from keras.losses import binary_crossentropy\n",
        "import keras.backend as K\n",
        "\n",
        "def dice_coeff(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "    return score\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    loss = 1 - dice_coeff(y_true, y_pred)\n",
        "    return loss\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "    return loss\n",
        "\n",
        "#========================================================================================\n",
        "# Neural network\n",
        "#========================================================================================\n",
        "import keras as keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, Activation, Concatenate, LeakyReLU, MaxPooling2D, UpSampling2D\n",
        "from keras import losses\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Expected input and output\n",
        "in_shape = x.shape[1:]\n",
        "out_shape = y.shape[1:]\n",
        "\n",
        "# How to set the initial random weights of Keras layers\n",
        "initializer = 'he_uniform'\n",
        "\n",
        "# Define the input layer:\n",
        "input_img = Input(shape=in_shape, name='input_img')\n",
        "\n",
        "# Encoder\n",
        "encoder = Conv2D(eN, kernel_size=3, strides=1, padding='same', kernel_initializer=initializer, name='ENC_CONV0')(input_img)\n",
        "encoder = LeakyReLU(alpha=0.1)(encoder)\n",
        "encoder = Conv2D(eN, kernel_size=3, strides=1, padding='same', kernel_initializer=initializer, name='ENC_CONV1')(encoder)\n",
        "encoder = LeakyReLU(alpha=0.1)(encoder)\n",
        "POOL1   = MaxPooling2D((2, 2), name='POOL1') (encoder)\n",
        "encoder = Conv2D(eN*2, kernel_size=3, strides=1, padding='same', kernel_initializer=initializer, name='ENC_CONV2')(POOL1)\n",
        "encoder = LeakyReLU(alpha=0.1)(encoder)\n",
        "POOL2   = MaxPooling2D((2, 2), name='POOL2') (encoder)\n",
        "encoder = Conv2D(eN*2, kernel_size=3, strides=1, padding='same', kernel_initializer=initializer, name='ENC_CONV3')(POOL2)\n",
        "encoder = LeakyReLU(alpha=0.1)(encoder)\n",
        "POOL3   = MaxPooling2D((2, 2), name='POOL3') (encoder)\n",
        "encoder = Conv2D(eN*4, kernel_size=3, strides=1, padding='same', kernel_initializer=initializer, name='ENC_CONV4')(POOL3)\n",
        "encoder = LeakyReLU(alpha=0.1)(encoder)\n",
        "POOL4   = MaxPooling2D((2, 2), name='POOL4') (encoder)\n",
        "encoder = Conv2D(eN*4, kernel_size=3, strides=1, padding='same', kernel_initializer=initializer, name='ENC_CONV5')(POOL4)\n",
        "encoder = LeakyReLU(alpha=0.1)(encoder)\n",
        "encoder = MaxPooling2D((2, 2), name='POOL5') (encoder)\n",
        "encoder = Conv2D(eN*8, kernel_size=3, strides=1, padding='same', kernel_initializer=initializer, name='ENC_CONV6')(encoder)\n",
        "encoder = LeakyReLU(alpha=0.1)(encoder)\n",
        "\n",
        "# Decoder\n",
        "decoder = UpSampling2D((2, 2), name='UPSAMPLE5')(encoder)\n",
        "decoder = keras.layers.concatenate([decoder,POOL4], name='CONCAT5')\n",
        "decoder = Conv2D(dN*16, kernel_size=3, strides=1, padding='same', kernel_initializer=initializer, name='DEC_CONV5A')(decoder)\n",
        "decoder = LeakyReLU(alpha=0.1)(decoder)\n",
        "decoder = Conv2D(dN*16, kernel_size=3, strides=1, padding='same', kernel_initializer=initializer, name='DEC_CONV5B')(decoder)\n",
        "decoder = LeakyReLU(alpha=0.1)(decoder)\n",
        "decoder = UpSampling2D((2, 2), name='UPSAMPLE4')(decoder)\n",
        "decoder = keras.layers.concatenate([decoder,POOL3], name='CONCAT4')\n",
        "decoder = Conv2D(dN*8, kernel_size=3, strides=1, padding='same', kernel_initializer=initializer, name='DEC_CONV4A')(decoder)\n",
        "decoder = LeakyReLU(alpha=0.1)(decoder)\n",
        "decoder = Conv2D(dN*8, kernel_size=3, strides=1, padding='same', kernel_initializer=initializer, name='DEC_CONV4B')(decoder)\n",
        "decoder = LeakyReLU(alpha=0.1)(decoder)\n",
        "decoder = UpSampling2D((2, 2), name='UPSAMPLE3')(decoder)\n",
        "decoder = keras.layers.concatenate([decoder,POOL2], name='CONCAT3')\n",
        "decoder = Conv2D(dN*6, kernel_size=3, strides=1, padding='same', kernel_initializer=initializer, name='DEC_CONV3A')(decoder)\n",
        "decoder = LeakyReLU(alpha=0.1)(decoder)\n",
        "decoder = Conv2D(dN*6, kernel_size=3, strides=1, padding='same', kernel_initializer=initializer, name='DEC_CONV3B')(decoder)\n",
        "decoder = LeakyReLU(alpha=0.1)(decoder)\n",
        "decoder = UpSampling2D((2, 2), name='UPSAMPLE2')(decoder)\n",
        "decoder = keras.layers.concatenate([decoder,POOL1], name='CONCAT2')\n",
        "decoder = Conv2D(dN*4, kernel_size=3, strides=1, padding='same', kernel_initializer=initializer, name='DEC_CONV2A')(decoder)\n",
        "decoder = LeakyReLU(alpha=0.1)(decoder)\n",
        "decoder = Conv2D(dN*4, kernel_size=3, strides=1, padding='same', kernel_initializer=initializer, name='DEC_CONV2B')(decoder)\n",
        "decoder = LeakyReLU(alpha=0.1)(decoder)\n",
        "decoder = UpSampling2D((2, 2), name='UPSAMPLE1')(decoder)\n",
        "decoder = keras.layers.concatenate([decoder,input_img], name='CONCAT1')\n",
        "decoder = Conv2D(64, kernel_size=3, strides=1, padding='same', kernel_initializer=initializer, name='DEC_CONV1A')(decoder)\n",
        "decoder = LeakyReLU(alpha=0.1)(decoder)\n",
        "decoder = Conv2D(32, kernel_size=3, strides=1, padding='same', kernel_initializer=initializer, name='DEC_CONV1B')(decoder)\n",
        "decoder = LeakyReLU(alpha=0.1)(decoder)\n",
        "decoder = Conv2D(1, (1, 1), activation='sigmoid')(decoder)\n",
        "\n",
        "# Single GPU\n",
        "model = Model(inputs=input_img, outputs=decoder, name=runID)\n",
        "model.summary()\n",
        "#plot_model(model, to_file='./Graphs/'+runID+'.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# Optimizer:\n",
        "optimizer = Adam(lr=lrate)\n",
        "\n",
        "# Compile model with specified loss function:\n",
        "model.compile(loss=bce_dice_loss, optimizer=optimizer)\n",
        "\n",
        "# Tensorboard:\n",
        "tbCallBack = keras.callbacks.TensorBoard(log_dir=runPath, histogram_freq=0, write_graph=True, write_images=True)\n",
        "\n",
        "# Checkpoints:\n",
        "checkpointCallBack = keras.callbacks.ModelCheckpoint(filepath=runPath+'/checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5', save_weights_only=True, verbose=1, period=50)\n",
        "\n",
        "# Test prediction result during training:\n",
        "def testPrediction(epoch):\n",
        "\tif epoch % 5 == 0:\n",
        "\t\tfor i in range(0,len(x),int(len(y)/5)):\n",
        "\t\t\tif epoch == 0:\n",
        "\t\t\t\tsave_png(x[i]*255,runPath+'/samples/i'+f'{i:04}'+'.0.rgb.png')\n",
        "\t\t\t\tsave_png(y[i]*255,runPath+'/samples/i'+f'{i:04}'+'.1.mask.png')\n",
        "\t\t\tprediction = predict(model, x[i])*255\n",
        "\t\t\tsave_png(prediction.reshape(resolution,resolution,3), runPath+'/samples/i'+f'{i:04}'+'.e'+f'{epoch:05}'+'.png')\n",
        "testPredictCallBack = keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: testPrediction(epoch))\n",
        "\n",
        "# Start training:\n",
        "history = model.fit(x, y, callbacks=[tbCallBack,checkpointCallBack,testPredictCallBack], epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
        "\n",
        "# Save trained model:\n",
        "model.save(runPath+'/model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwsrMdHsK1hE"
      },
      "source": [
        "# Test the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mVYL8s6SfJb"
      },
      "source": [
        "imgs, masks = generate_random_location(location, actor, danger, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Byo_FckFJpSp"
      },
      "source": [
        "from skimage.transform import resize\n",
        "idx = 3\n",
        "\n",
        "input = resize(numpy.array( imgs[idx] ), (128,128), anti_aliasing=True)\n",
        "output = predict(model, input)\n",
        "\n",
        "figure(); imshow(input)\n",
        "figure(); imshow(output[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
